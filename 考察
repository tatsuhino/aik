対象とするデータは、検索途中のコンテンツなのか、求めていたコンテンツなのかが明確に分かれている必要がある。
その二つの条件を満たすデータとして、kaggleで公開されていたECサイトのデータを利用した。
検索途中のコンテンツ⇒View,求めていたコンテンツ⇒addToCart


■処理プロセス

検索途中のコンテンツ⇒View,求めていたコンテンツ⇒addToCart


■データ収集
・データ取得＞整形＞モデル構築＞評価の順で処理を行う。
・対象とするデータは、検索途中のコンテンツなのか、求めていたコンテンツなのかが明確に分かれている必要がある。
その二つの条件を満たすデータとして、kaggleで公開されていたECサイトのデータを利用した。

■整形
https://www.slideshare.net/ShunNukui/ss-94997937
・公開データの形式のままでは利用できないので、履歴順になるように整形
・レコメンド問題のよくある設定として、user,itemに対して良い、悪いのスコアが与えられている。
暗黙的フィードバックをスコアリング　同一のユーザがなどもリピートしていれば高スコア

■モデル構築​
ベクトルの次元数、エポック数などを比較し、最も高い結果となったものを採用。
自然言語処理の場合は次元数400で精度が上がる
200次元が維持版好スコアとなった。

■評価​
・ECサイトのデータを使用し、ユーザの評価履歴に対して、ゴールのデータがどれだけ充てられるかを、
協調フィルタリング、Doc2vecを利用した場合で比較した。

データ分割して、交差検証した。
何をもってHITと判断したのかの説明。上位10件

■結果棒グラフ

結果	結果	"協調フィルタリング＜Doc2Vec？の結果になった。
ただし、Doc2Vecの場合でもそこまで確率は高くなかった。
"	"以下の３パターンで精度を比較
・よく買われているアイテムの上位何件
・協調フィルタリングのユーザベース
・Doc2Vec
・Doc2Vec(アイテムベース


■考察
協調フィルタリング：
メモリ

Doc2Vec：
モデルを可視化したら、最初の仮説が怪しいことが見えてきた。
一方で、ベクトルの次元数が大幅に少ないため、少なりリソースでリアルアイム処理をする場合は有効
（協調フィルタリングの場合は32GBでもメモリが不足。。）

データを精査して可視化してみると、バラバラだったことが分かった。
本当に欲しい、ゴールにたどり着くまでの

■今後の課題
①の中での振り返り

②をするにあたっての課題
ナレッジ検索では、検索途中のコンテンツなのか、求めていたコンテンツなのかがそのままでは明確に判断つかないので、閲覧時間や閲覧回数、いいねボタンなどで判断する必要がある。
コールドスタート問題→コンテンツベースのレコメンドを

■研究を通じて学んだこと

成果：機械学習をする上での処理プロセスを一通り学んだ。
基本的なレコメンドアルゴリズムについて知識を身に着けた。


データの精査を早い段階でやること（それをするためにも可視化が大事。
割と簡単。pythonも今回初めて触れたが、とっつきやすい。一般的によくやられている処理についてはライブラリも充実しているのですぐできる。
一方で、新しいことをやろうとした場合だったり、細かいチューニングをする場合はやっぱり知見がないと厳しい。
数学的な背景を知らないままでは、あるものを使うことしかできない。



